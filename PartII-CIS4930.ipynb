{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T18:23:16.594651Z",
     "start_time": "2020-11-07T18:23:16.289650Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "img_dir = \"face_images/*\"\n",
    "files = glob.glob(img_dir)\n",
    "data = []\n",
    "\n",
    "for f1 in files:\n",
    "    img = cv2.imread(f1)\n",
    "    data.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T18:55:20.239168Z",
     "start_time": "2020-11-07T18:55:20.218670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image type: torch.FloatTensor\n",
      "Image size: torch.Size([751, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "#https://www.cs.virginia.edu/~vicente/recognition/notebooks/image_processing_lab.html\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# pytorch provides a function to convert PIL images to tensors.\n",
    "pil2tensor = transforms.ToTensor()\n",
    "tensor2pil = transforms.ToPILImage()\n",
    "\n",
    "\n",
    "# Read the image from file. Assuming it is in the same directory.\n",
    "pil_image = Image.open('face_images/image00000.jpg')\n",
    "rgb_image = pil2tensor(pil_image)\n",
    "rgb_image_nimages = rgb_image[None, :, :, :]\n",
    "rgb_image_nimages = rgb_image_nimages[np.zeros(len(data)), :, :, :]\n",
    "\n",
    "\n",
    "# Plot the image here using matplotlib.\n",
    "def plot_image(tensor):\n",
    "    plt.figure()\n",
    "    # imshow needs a numpy array with the channel dimension\n",
    "    # as the the last dimension so we have to transpose things.\n",
    "    plt.imshow(tensor.numpy().transpose(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "# plot_image(rgb_image_nimages)\n",
    "\n",
    "# Show the image tensor type and tensor size here.\n",
    "print('Image type: ' + str(rgb_image_nimages.type()))\n",
    "print('Image size: ' + str(rgb_image_nimages.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T18:56:04.817292Z",
     "start_time": "2020-11-07T18:56:04.804293Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def resize_tensor(input_tensors, h, w):\n",
    "    final_output = None\n",
    "    batch_size, channel, height, width = input_tensors.shape\n",
    "    input_tensors = torch.squeeze(input_tensors, 1)\n",
    "\n",
    "    for img in input_tensors:\n",
    "\n",
    "        img_PIL = transforms.ToPILImage()(img)\n",
    "        img_PIL = torchvision.transforms.Resize([h,w])(img_PIL)\n",
    "        img_PIL = torchvision.transforms.ToTensor()(img_PIL)\n",
    "\n",
    "        if final_output is None:\n",
    "            final_output = img_PIL\n",
    "\n",
    "        else:\n",
    "\n",
    "            final_output = torch.cat((final_output, img_PIL), 0)\n",
    "        final_output = torch.unsqueeze(final_output, 1)\n",
    "\n",
    "    return final_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T18:56:06.892866Z",
     "start_time": "2020-11-07T18:56:06.464366Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 4 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-c882a503e5bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresize_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb_image_nimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-62-29553181659a>\u001b[0m in \u001b[0;36mresize_tensor\u001b[1;34m(input_tensors, h, w)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mfinal_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_PIL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mfinal_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 3"
     ]
    }
   ],
   "source": [
    "size = resize_tensor(rgb_image_nimages,64,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T19:17:16.149061Z",
     "start_time": "2020-11-07T19:17:16.134061Z"
    }
   },
   "outputs": [],
   "source": [
    "input_tensors = rgb_image_nimages\n",
    "final_output = None\n",
    "batch_size, channel, height, width = input_tensors.shape\n",
    "input_tensors = torch.squeeze(input_tensors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T19:17:16.629561Z",
     "start_time": "2020-11-07T19:17:16.615562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([751, 3, 128, 128])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
